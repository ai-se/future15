
\documentclass[journal]{IEEEtran} 
\usepackage{graphicx}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}    
\begin{document} 
\title{Software Engineering in 2050}%
\author{Tim Menzies, James Whitehead% <-this % stops a space
\thanks{T. Menzies is with Computer Science,
North Carolina State University, North Carolina, USA, 27616 e-mail: tim.menzies@gmail.com).}% <-this % stops a space
\thanks{J. Whitehead with Computer Science, Dept. of Computational Media,
University of California, CA, 95064 e-mail: ejw@cs.ucsc.edu}}%  
 
\maketitle

\begin{abstract}
%\boldmath
Imagine we could travel forward in time to 2050. What would
we see there? 
How, and why, would software engineering be different then 
than now? What disruptions could we predict,
given recent research results?
\end{abstract}

Looking back from 2050, the current landscape of software engineering
looms imaginably different to the status quo, even 30 years ago. So much has changed including the end of social and the rise of privacy, the end of small data and the era of small data,
the rise of collapse predictors, the end of agile, the rise of the automatic programming and  AIs.
In this short article we pretend we are explaining to a reader in 2015 just how much of their then current thinking about software will become so rapidly and completely outdated.

\section{The Collapse of Social}

The biggest change in
software development is, of course,   the Swarms. It i would be hard to explain to someone from 2015 how swarm of small software teams changed the nature of software production, and indeed all of human organization. 

The rise of the Swarms was inevitable, once all of India and all or Africa came on-line.  The ready availability of the this large group of programmers coincided the discovery of effective next-generation crowd-sourcing technologies.  
While early experiments in crowd-sourcing were generally less-than-successful,
several factors lead to widespread adoption of swarm-based development. 
Research of Yang et al. showed that code developed in this crowd-sourced manner can be up to five times cheaper
than using conventional approaches (NIER13). 
Other work by  Bird et al (icse09, incse12) showed product-based management hierarchies (rather than locality-based management hierarchies) let teams around the world collaborate effectively on large software projects.  Subsequent work by LaToza et al. (NIER15) showed that large tasks can be divided into many ``micro tasks''
that individuals can complete in their spare minutes between other activities
(e.g. critique the color choices on this screen;  check if these tests cover the important branches of this function).  

Of course, the crowd cannot work on many parts of a system unless all those
parts are first identified. Once researchers had access to large source code repositories (initially, in systems like Mozilla and, subsquently, in systems
like Github), it became possible to  automatically identify the large scale patterns used to structure software. The work of Devanbu et al. (NIER13) was particularly important in this regard.  After reviewing billions of lines of code in Github, Devanbu discovered that most code repeats previously written code. That is, for much of our systems, once some small part is developed then those code fragments can be used to retrieve large sections of related support code from prior development. After that, systems development becomes a matter of ``stiching'' together all those fragments-- a task that can be readily divided, parcelled up, and sent to the Swarms.  Decades of Swarm-based development
has shown that 

Not all code can be shared to the entire world such as  the launch codes for the
  nuclear missiles maintained by the Asteroid Defense Authority or the 
  attitude control software for the orbiting mircowave power relay stations. 
  Accordingly, different Swarms work for different communities. Econcomists
  now agree that nation states are only viable if their indigenous population
  is large enough to support a Swarm that is (a)~loyal to that nation and (b)~big enough to work behind the firewalls supporting that nation's  software infrastructure.
  
  



crowd sourcing experiments were 


Once     
semi-quantum computers became widely available in the 2020s, anyone could
crack most encryption systems.   This lead to the end of the era of Big
Data since it was no longer possible to collect and secure large repositories
of information.

Faced with the collapse of social networks, industry leaders scrambled
to find new kinds of software services to sell to an acutely privacy-aware
community.
The story of the Zuckerburg/Gates weekend meeting of 2020 is now legendary.
Challenged by Gates to do something to really change the world, Zuckerburg published his famous howl of protest at the excess of effort being devoted to
social media:
\begin{quote}
(With apologies to Ginsberg) I saw the best minds of my generation destroyed by ad revenue madness, sipping hysterical Starbucks, dragging themselves through optimizations of click-throughs, looking for an Google buy-out. Angel-headed hackers (and their backers) burning for the ancient heavenly connection to the cloud-based dynamo in the machinery of the Internet, trillions of shiny sparks on their misdirected screens.
\end{quote}
Vowing to better focus the intellectual efforts of the planet, 
Zuckerberg devoted a large portion of his considerable wealth to the UnCovered
project where humans hide their identity  behind avatars and each avatar is
scored ``up'' or ``down'' after each interaction with others. As the reputation of some avatars grew, their ``good name'' became a marketable  commodity (for example, the Gandhi avatar recently sold for \$1B). Newbies declined to interact with ``down'' avatars, choosing instead to only interact with folks who are know to play well with others. 

UnCovered's motto of ``careful, someone might see you'' became the core assumption of the computer and software industry. UnCovered's  data mining tools  learn  patterns of ``predators'' (the marketing bots trying to find  patterns in the buying behavior of individuals) and advise users on how to change
their behaviour in order to  ``cover their tracks''.
Without a way to target advertising, click through revenues dried up and once-might giants like Google were broken up and sold off. 


Organizational structures followed suite. Once an organization grew large enough to call attention to itself, it then fell apart under the weight of thousands of cyperattacks. Large companies
like  IBM and Oracle were replacing with ``swarms''-
large crowd source activities where multiple teams compete to finish tasks specified in the ``whim of the day'' messages from   the avatars
with highest ``good name''.

In fact, 

who could have imagined in 2015 that this approach would swell and replace traditional corporate structures? After some intial 



With no way to tag and trace single people, it became impossible to tune ads to individuals.  As the click through revenue streams dried up, organizations like Google




denila of motivation attacks

denial of interest

social compilers-- social engineering to generate the community to build
extend and maintain the system. 

the social wars

%\IEEEpeerreviewmaketitle
Humans out of the loop: humans out of the loop. four parts:
\bi
\item Planning
\item Coding
\bi
\item recommender system to fill in stuff
\item natural code to fill in
\ei
\item Testing
\bi
\item open source told us large test community. issue is focusing them. gamification
\item auto test repair . claire wes
\item auto test oracles: ai for better partitioning, features section fo irreleancies.
taken advanate of test case programs. 
\ei
\ei

pattern discovery from large corpuses. coding suggestion. lauguage design from AIs 

\bi
\item language generator generator
\item reinforcement learning. chunking mechansim SOAR to learn new language patterns/shortcuts
\item explanation-based generation
\ei

collapses of the 20s and 30s and excess of big data big brother
\bi
\item military failures of city vs nomad economies
\item larger systems collapse harder, harder to evolve and maintain
\item lesson : local islands of order
\ei

large systems that have large logs
\bi
\item really long supply chains brittle . not longer and larger, but
smaller and easier to maintain. monetize disruption. no more 787 .
no more centralized electrical generation.
but lots of cars
\item feature selection
\item instance selection
\item destruction: the right people right tweaks
\ei

forced into smaller systems

open source battlefield- suck the value of the idea. always providing value above and beyond
the the. the android effects. other companies disrupt them. denial of motivation attacks. realized software are sociotechnological terrorism. ace developers have to become anonymous.

mexico- want to not be prominent. extortion rackets 

out sourcing: portable security enviornments

micro division, micro tasks. not chief programmer, ibm ants.

information, the more organized the item, the more energy 
\bi
\item the cleverer, the more organized, the more resources
\item n highly organized systems the more complex the interfaces to other systems that evolve
\item economics is the way we deal with entropy when 
\ei

scope much broader. intro example no longer hello world but hello worldClimateModel. e.g. loop pattern to loop invariant

\section{End of Social}


\section{Collapse Predictors}

Whitehead, msr15, possible to predict periods of steady growth
as well as impending sudden change in software systems and
ecosystems. Faced with impending collapse:
\bi
\item Venture capitalists withdraw funds;
\item Programmers start relocating in droves;
\ei
 

\section{No more Big Data}

\subsection{The small data revolution}

Peters (icse15) showed that micro sharing allows a community to share small (and somewhat obsficated) data, and predictions from the shared data are better than using all the unobfuscated data (why? cause fayola's obsficates via   averages over instance space so her 
unobfuscated examples contain more information that raw data)

\section{Auto coding}

Prem's naturalness results means that if we can access large samples of code,
then we will find many repeated patterns. Which means that if we starting coding some that we think is new, an whole host of recommendor  systems should soon   wake up to propose large additional code chunks and/or test suites. 

Which changes the nature of programming. instead coding go to whoa, we now work on a "hint" basis, where given any problem, we start with the most unique aspect first. after which ai agents start proposing relevant additions or (if we deviate from usual practice) cautions when we leave the mainstream

one side effect of this was to stem the tide of new languages. in 2015, there is a the MLOC rule: not new programming language can be generally used until there is at least 10 million LOC written in that code.  

\subsection{The Great CPU Cooldown}

{\em not sure on this one...}

Ever since the melting of the Greenland icepack (with its associated disastrous impacts on the former European nations), there has been an increasing emphasis on sheppherding existing resources rather than assumption some exponentially increasing resource pool. Now new techniques are required to show that they consume fewer resources than prior innovations. 



disasters like CRATER become more common (the micrometeorite model that said "columbia ice strike? no worries! even though that model was for far slower/smaller things that what hit columbia). program test suites summarized by data miners as "certification envelopes" that ship with a system reporting when that system is going outside the context where it was tested.

context and locality results changed the nature of science. if everything local then no general models. however, there are cost effective ways of building local models. so it is not everyone civics duty to monitor old policies to see if we ahve moved to a zone where the old no longer applies.

\section{Group Think Fuck (must change this title)}

after the collapse of american public funding for research, the
perverse effect of competition on researchers lead to a shell game where folks hide their stuff http://link.springer.com/article/10.1007/s11948-007-9042-5

in response to this, academics evolvted challenge partners.
sort of kinda promise$^2$

end of papers, unapproachable diamonds written by acadamics devoted
to enshrningin particular ideas

more challenge problems generated by industry and lots of small tools dropping
back from academ back to industy



\section{Outsourcing to the next Billion}

Early experiments, out-souricing. suck. then discoverred
that our-sourced development more regular and  predictable (ye yang, msr 2013) so managers turned to out sourcing to reduce risk in
software 

\section{Intrinsic Dimensionality}

As we studied more code, a repeated result is that much of that data is superflous, that the metrics we see in code are surface features, echoes of an underlying multi-dimensional structure. Much success in spectral methods for exploiting that underlying structure. Which means much of the old work on metrics simply faded away.

\section{anomalies and locality}


\section{Sentient AI}

All the SBSE stuff (e.g. claire degouse et al icse12 etc)

Take the cloud to some place more energy full (floating nearer the sun) but less hospitable to humans. Software {\em herding} more
than software {\em programming}. Turn lose a herd of AI algorithms
that mutate known current solutions to generate multiple
alternative candidate solutions.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.



\section{Conclusion}
 
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
 

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:
 

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}


\bibliography{refs}

% that's all folks
\end{document}


