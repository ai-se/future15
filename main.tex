
\documentclass[journal]{IEEEtran} 
\usepackage{graphicx}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}    
\begin{document} 
\title{Software Engineering in 2050}%
\author{Tim Menzies, James Whitehead% <-this % stops a space
\thanks{T. Menzies is with Computer Science,
North Carolina State University, North Carolina, USA, 27616 e-mail: tim.menzies@gmail.com).}% <-this % stops a space
\thanks{J. Whitehead with Computer Science, Dept. of Computational Media,
University of California, CA, 95064 e-mail: ejw@cs.ucsc.edu}}%  
 
\maketitle

\begin{abstract}
  %\boldmath
  Looking back from 2050, the current landscape of
software engineering is so unimaginably different to
the status quo, even 30 years ago.  In this short article we pretend we are
explaining to a reader in 2015 just how much of
their then current thinking about software 
became so rapidly and completely outdated.
\end{abstract}



\section{Swarm Programming}

The biggest change in software development is, of
course, the Swarms. It would be hard to explain to
someone from 2015 how swarm of small software teams
changed the nature of software production, and
indeed all of human organization.

The rise of the Swarms was inevitable, once all of
India and all or Africa came on-line.  The ready
availability of the this large group of programmers
coincided the discovery of effective next-generation
crowd-sourcing technologies.  While early
experiments in crowd-sourcing were generally
less-than-successful, several factors lead to
widespread adoption of swarm-based development:
\bi
\item
Research of Yang et al. showed that code developed
in this crowd-sourced manner can be up to five times
cheaper than using conventional approaches (NIER13).
\item
Other work by Bird et al (icse09, incse12) showed
product-based management hierarchies (rather than
locality-based management hierarchies) let teams
around the world collaborate effectively on large
software projects.
\item
  Subsequent work by LaToza et
al. (NIER15) showed that large tasks can be divided
into many ``micro tasks'' that individuals can
complete in their spare minutes between other
activities (e.g. critique the color choices on this
screen; check if these tests cover the important
branches of this function).
\ei
Of course, the crowd cannot work on many parts of a
system unless all those parts are first
identified. Once researchers had access to large
source code repositories (initially, in systems like
Mozilla and, subsquently, in systems like Github),
it became possible to automatically identify the
large scale patterns used to structure software. The
work of Devanbu et al. (NIER13) was particularly
important in this regard.  After reviewing billions
of lines of code in Github, Devanbu discovered that
most code repeats previously written code. That is,
for much of our systems, once some small part is
developed then those code fragments can be used to
retrieve large sections of related support code from
prior development. After that, systems development
becomes a matter of ``stiching'' together all those
fragments-- a task that can be readily divided,
parcelled up, and sent to the Swarms.

This style of development lead to new paradigms of program development.
``Stiching'' only works when there are
materials that can be stitched together. Hence,
before any new language can become widely used, it
must pass through:
\begin{itemize}
\item
  A ``sermon'' phase where some guru
argues that some
new formalism is a better way to tackle software projects.
\item
Hopefully, sermons are followed by a ``coding 1000''
phase where a community advocating for that language
writes examples and tutorials about implementing the
standard 1000 most common programming tasks
(e.g. search a string, update a window, build an AI,
drive a 3D printer to build all the parts needed for
another 3D printer, simulate a world climate
model).
\item
  If that is successful, commercial
programming teams start ``porting'' old code into
the new formalism (with the hope that subsequent
``stichers'' will pay them a small license fee for
the rights to use their code).
\ei
Not all code can be shared to the entire world (e.g. such
as the launch codes for the nuclear missiles
maintained by the Asteroid Defense Authority or the
attitude control software for the orbiting mircowave
power relay stations).  Accordingly, specialized Swarms
work just within specialized communities. Economists now
agree that nation states are only viable if their
indigenous population is large enough to support a
Swarm that is (a)~loyal to that nation and (b)~big
enough to work behind the firewalls supporting that
nation's software infrastructure.

Swarm dynamics is an active field of research. For
many decades, Whitehead et al's old power laws
(MSR'15) were a useful tool for predicting the rise
and fall of swarm communities. Temporal data miners
were used to match the growth rate of a programming
swarm to the ``foothills'' of Whitehead's power law
distributions\footnote{Whitehead et al.
  showed that a surprisingly large
  number of temporal growth patterns
  in software repotories.  Once the start of a exponential
growth was detected, this attracted venture capital
money which, in turn, attracted more swam
programmers (thus feeding the growth rate even
more).  Similarly, once the temporal data miners
start sensing an impending ``collapse'' phase, then
venture capital and programmers abandon that
community en masse (thus hastening its collapse).

The whole field of ``social compilations''
would be unimaginable by a software engineering from 2015.
We cannot see how to explain to someone from that era
how social engineers now nudge communities to build
and maintain Swarms and their associated software.

Of course, social compilation has a darker side--
the design of so-called ``motivational
attacks'' where opposing swarms encourage thought
leaders to make disparaging about opposing Swarms,
thus inhibiting their growth or hastening their
demise.  Motivational attacks
have been shown to be
surprisingly effective- a mere 1\% difference in the
early growth rate of a Swarm can delay them so much
that in a single year, opposing Swarms can capture
entire markets before the slower Swarms can catch
up. 
Defense departments around the world are now
gearing up for the expected coming of ``social wars''
where one nation tries to bankrupt another via
motivational attacks to convince their
Swarm programmers that that  to move on,
just to avoid some imagined Swarm collapse.

\section{AI Rising}

The other big change from 2015 is of course the rise
of AI-based software engineering. Initial
experiments in the 20th with automated programming
and artificial intelligence ran into the
axiom-authoring problem; i.e. how to encode enough
background knowledge to enable subsequent reasoning.
By 2020, that problem had all but disappeared. Once
AIs could access very large set of coding examples,
they could then infer the structures and patterns
that repeat within those systems.

These days, we would not use AIs to write the
initial version of some software system. Experience
has shown that we can learn most about user goals
and domain constraints from some initial prototype
than any auto-generated construct.






Once     
semi-quantum computers became widely available in the 2020s, anyone could
crack most encryption systems.   This lead to the end of the era of Big
Data since it was no longer possible to collect and secure large repositories
of information.

Faced with the collapse of social networks, industry leaders scrambled
to find new kinds of software services to sell to an acutely privacy-aware
community.
The story of the Zuckerburg/Gates weekend meeting of 2020 is now legendary.
Challenged by Gates to do something to really change the world, Zuckerburg published his famous howl of protest at the excess of effort being devoted to
social media:
\begin{quote}
(With apologies to Ginsberg) I saw the best minds of my generation destroyed by ad revenue madness, sipping hysterical Starbucks, dragging themselves through optimizations of click-throughs, looking for an Google buy-out. Angel-headed hackers (and their backers) burning for the ancient heavenly connection to the cloud-based dynamo in the machinery of the Internet, trillions of shiny sparks on their misdirected screens.
\end{quote}
Vowing to better focus the intellectual efforts of the planet, 
Zuckerberg devoted a large portion of his considerable wealth to the UnCovered
project where humans hide their identity  behind avatars and each avatar is
scored ``up'' or ``down'' after each interaction with others. As the reputation of some avatars grew, their ``good name'' became a marketable  commodity (for example, the Gandhi avatar recently sold for \$1B). Newbies declined to interact with ``down'' avatars, choosing instead to only interact with folks who are know to play well with others. 

UnCovered's motto of ``careful, someone might see you'' became the core assumption of the computer and software industry. UnCovered's  data mining tools  learn  patterns of ``predators'' (the marketing bots trying to find  patterns in the buying behavior of individuals) and advise users on how to change
their behaviour in order to  ``cover their tracks''.
Without a way to target advertising, click through revenues dried up and once-might giants like Google were broken up and sold off. 


Organizational structures followed suite. Once an organization grew large enough to call attention to itself, it then fell apart under the weight of thousands of cyperattacks. Large companies
like  IBM and Oracle were replacing with ``swarms''-
large crowd source activities where multiple teams compete to finish tasks specified in the ``whim of the day'' messages from   the avatars
with highest ``good name''.

In fact, 

who could have imagined in 2015 that this approach would swell and replace traditional corporate structures? After some intial 



With no way to tag and trace single people, it became impossible to tune ads to individuals.  As the click through revenue streams dried up, organizations like Google




denila of motivation attacks

denial of interest

social compilers-- social engineering to generate the community to build
extend and maintain the system. 

the social wars

%\IEEEpeerreviewmaketitle
Humans out of the loop: humans out of the loop. four parts:
\bi
\item Planning
\item Coding
\bi
\item recommender system to fill in stuff
\item natural code to fill in
\ei
\item Testing
\bi
\item open source told us large test community. issue is focusing them. gamification
\item auto test repair . claire wes
\item auto test oracles: ai for better partitioning, features section fo irreleancies.
taken advanate of test case programs. 
\ei
\ei

pattern discovery from large corpuses. coding suggestion. lauguage design from AIs 

\bi
\item language generator generator
\item reinforcement learning. chunking mechansim SOAR to learn new language patterns/shortcuts
\item explanation-based generation
\ei

collapses of the 20s and 30s and excess of big data big brother
\bi
\item military failures of city vs nomad economies
\item larger systems collapse harder, harder to evolve and maintain
\item lesson : local islands of order
\ei

large systems that have large logs
\bi
\item really long supply chains brittle . not longer and larger, but
smaller and easier to maintain. monetize disruption. no more 787 .
no more centralized electrical generation.
but lots of cars
\item feature selection
\item instance selection
\item destruction: the right people right tweaks
\ei

forced into smaller systems

open source battlefield- suck the value of the idea. always providing value above and beyond
the the. the android effects. other companies disrupt them. denial of motivation attacks. realized software are sociotechnological terrorism. ace developers have to become anonymous.

mexico- want to not be prominent. extortion rackets 

out sourcing: portable security enviornments

micro division, micro tasks. not chief programmer, ibm ants.

information, the more organized the item, the more energy 
\bi
\item the cleverer, the more organized, the more resources
\item n highly organized systems the more complex the interfaces to other systems that evolve
\item economics is the way we deal with entropy when 
\ei

scope much broader. intro example no longer hello world but hello worldClimateModel. e.g. loop pattern to loop invariant

\section{End of Social}


\section{Collapse Predictors}

Whitehead, msr15, possible to predict periods of steady growth
as well as impending sudden change in software systems and
ecosystems. Faced with impending collapse:
\bi
\item Venture capitalists withdraw funds;
\item Programmers start relocating in droves;
\ei
 

\section{No more Big Data}

\subsection{The small data revolution}

Peters (icse15) showed that micro sharing allows a community to share small (and somewhat obsficated) data, and predictions from the shared data are better than using all the unobfuscated data (why? cause fayola's obsficates via   averages over instance space so her 
unobfuscated examples contain more information that raw data)

\section{Auto coding}

Prem's naturalness results means that if we can access large samples of code,
then we will find many repeated patterns. Which means that if we starting coding some that we think is new, an whole host of recommendor  systems should soon   wake up to propose large additional code chunks and/or test suites. 

Which changes the nature of programming. instead coding go to whoa, we now work on a "hint" basis, where given any problem, we start with the most unique aspect first. after which ai agents start proposing relevant additions or (if we deviate from usual practice) cautions when we leave the mainstream

one side effect of this was to stem the tide of new languages. in 2015, there is a the MLOC rule: not new programming language can be generally used until there is at least 10 million LOC written in that code.  

\subsection{The Great CPU Cooldown}

{\em not sure on this one...}

Ever since the melting of the Greenland icepack (with its associated disastrous impacts on the former European nations), there has been an increasing emphasis on sheppherding existing resources rather than assumption some exponentially increasing resource pool. Now new techniques are required to show that they consume fewer resources than prior innovations. 



disasters like CRATER become more common (the micrometeorite model that said "columbia ice strike? no worries! even though that model was for far slower/smaller things that what hit columbia). program test suites summarized by data miners as "certification envelopes" that ship with a system reporting when that system is going outside the context where it was tested.

context and locality results changed the nature of science. if everything local then no general models. however, there are cost effective ways of building local models. so it is not everyone civics duty to monitor old policies to see if we ahve moved to a zone where the old no longer applies.

\section{Group Think Fuck (must change this title)}

after the collapse of american public funding for research, the
perverse effect of competition on researchers lead to a shell game where folks hide their stuff http://link.springer.com/article/10.1007/s11948-007-9042-5

in response to this, academics evolvted challenge partners.
sort of kinda promise$^2$

end of papers, unapproachable diamonds written by acadamics devoted
to enshrningin particular ideas

more challenge problems generated by industry and lots of small tools dropping
back from academ back to industy



\section{Outsourcing to the next Billion}

Early experiments, out-souricing. suck. then discoverred
that our-sourced development more regular and  predictable (ye yang, msr 2013) so managers turned to out sourcing to reduce risk in
software 

\section{Intrinsic Dimensionality}

As we studied more code, a repeated result is that much of that data is superflous, that the metrics we see in code are surface features, echoes of an underlying multi-dimensional structure. Much success in spectral methods for exploiting that underlying structure. Which means much of the old work on metrics simply faded away.

\section{anomalies and locality}


\section{Sentient AI}

All the SBSE stuff (e.g. claire degouse et al icse12 etc)

Take the cloud to some place more energy full (floating nearer the sun) but less hospitable to humans. Software {\em herding} more
than software {\em programming}. Turn lose a herd of AI algorithms
that mutate known current solutions to generate multiple
alternative candidate solutions.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.



\section{Conclusion}
 
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
 

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:
 

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}


\bibliography{refs}

% that's all folks
\end{document}


